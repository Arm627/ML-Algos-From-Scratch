{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><b>Implementing Softmax Regression (Multinomial Logistic Regression)</b></h3>\n",
    "<h5 style=\"text-align: center;\">This notebook follows this Stanford file: <a href=\"http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/\" target=\"_blank\">http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/</a><br>and this tutorial by Nikhil Kumar:<a href=\"https://www.geeksforgeeks.org/softmax-regression-using-tensorflow/\" target=\"_blank\">https://www.geeksforgeeks.org/softmax-regression-using-tensorflow/</a></h5>\n",
    "<h5 style=\"text-align: center;\">**Again like the last notebook, I recomend just following those two tutorials above. This notebook doesnt add much to both those wonderful tutorials</h5>\n",
    "$$ \\text{Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes.} \\\\ \\text{In logistic regression we assumed that the labels were binary: } y_i = \\in \\{0, 1\\} $$\n",
    "$$ \\text{Softmax regression allows us to handle: } y_i = \\in \\{0, 1, \\ldots, K\\} \\text{where K is the number of classes.} $$\n",
    "$$ \\text{Given a test input x, we want our hypothesis to estimate the probability that } P(y=k|x) \\\\ \\text{ for each value of } k = 1,\\ldots,K \\text{. I.e., we want to estimate the probability of the class label taking on each of the K different possible values. Thus, our hypothesis will output a } \\\\ \\text{K-dimensional vector (whose elements sum to 1) giving us our K estimated probabilities.} $$\n",
    "$$ \\text{Let } X = \\begin{equation} \\begin{bmatrix} 1 & x_{1,1} & \\ldots & x_{1,p} \\\\ 1 & x_{2,1} & \\ldots & x_{2,p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n,1} & \\ldots & x_{n,p} \\end{bmatrix} \\label{eq:aeqn} \\end{equation}$$\n",
    "$$ \\text{Where the dataset has ‘m’ features and ‘n’ observations. Also, there are ‘k’ class labels, i.e every observation can be classified} \\\\ \\text{as one of the ‘k’ possible target values. For example, if we have a dataset of 100 handwritten digit images of vector size 28×28 for} \\\\ \\text{digit classification, we have, n = 100, m = 28×28 = 784 and k = 10.} $$\n",
    "$$ \\text{Then our prediction function is: } P(y_i=k|x_i;\\theta) = \\begin{bmatrix} P(y=0|x;\\theta) \\\\ P(y=1|x;\\theta) \\\\ \\vdots \\\\ P(y=K|x;\\theta) \\end{bmatrix} = \\frac{e^{\\theta_k^Tx_i}}{\\sum_{j=1}^{K}e^{\\theta_j^Tx}}$$\n",
    "<h5 style=\"text-align: center;\">We now describe the cost function that we’ll use for softmax regression. In the equation below, 1{⋅} is the ”‘indicator function,”’ so that 1{a true statement}=1, and 1{a false statement}=0. For example, 1{2+2=4} evaluates to 1; whereas 1{1+1=5} evaluates to 0. Our cost function will be:</h5>\n",
    "$$ J(\\theta) = -[\\sum_{i=1}^{n}\\sum_{k=1}^{K}1\\{y_i=k\\}\\log_{10}(\\frac{e^{\\theta_k^Tx_i}}{\\sum_{j=1}^{K}e^{\\theta_j^Tx_i}})] $$\n",
    "<h5 style=\"text-align: center;\">Notice that this generalizes the logistic regression cost function, which could also have been written:</h5>\n",
    "$$ J(\\theta) = -[\\sum_{i=1}^{n}\\log_{10}(1 - h_\\theta(x_i)) + y_i(\\log_{10}(h_\\theta(x_i))] $$\n",
    "$$ = -[\\sum_{i=1}^{n}\\sum_{k=0}^{1}1\\{y_i=k\\}\\log_{10}(P(y_i=k|x_i;\\theta)] $$\n",
    "<h5 style=\"text-align: center;\">We cannot solve for the minimum of J(θ) analytically, and thus as usual we’ll resort to an iterative optimization algorithm. Taking derivatives, one can show that the gradient is:</h5>\n",
    "$$ \\triangledown_{\\theta_k}J(\\theta) =  -\\sum_{i=1}^{n}[x_i(1\\{y_i=k\\} - P(y_i=k|x_i;\\theta)] \\text{ or: } $$\n",
    "$$ \\triangledown_{\\theta_k}J(\\theta) =  -\\sum_{i=1}^{n}[x_i(1\\{y_i=k\\} - \\frac{e^{\\theta_k^Tx_i}}{\\sum_{j=1}^{K}e^{\\theta_j^Tx}}] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def loadCSV(filename): \n",
    "    ''' \n",
    "    function to load dataset \n",
    "    '''\n",
    "    with open(filename,\"r\") as csvfile: \n",
    "        lines = csv.reader(csvfile) \n",
    "        dataset = list(lines) \n",
    "        for i in range(len(dataset)): \n",
    "            dataset[i] = [float(x) for x in dataset[i]]      \n",
    "    return np.array(dataset) \n",
    "\n",
    "def normalize(X): \n",
    "    ''' \n",
    "    function to normalize feature matrix, X \n",
    "    '''\n",
    "    mins = np.min(X, axis = 0) \n",
    "    maxs = np.max(X, axis = 0) \n",
    "    rng = maxs - mins \n",
    "    norm_X = 1 - ((maxs - X)/rng) \n",
    "    return norm_X \n",
    "\n",
    "def P(theta, X):\n",
    "    #return np.exp(np.matmul(theta.T, X)) / np.sum(np.exp(np.matmul(theta.T, X)))\n",
    "    return np.exp(theta.T * X) / np.sum(np.exp(theta.T * X))\n",
    "\n",
    "def J(theta, X, Y, K):\n",
    "    J = 0\n",
    "    for i in range(len(Y)):\n",
    "        for k in range(1, K):\n",
    "            if Y[i] == k:\n",
    "                multiplier = 1\n",
    "            else:\n",
    "                multiplier = 0\n",
    "            J += multiplier*np.log10(P(theta, X))\n",
    "    return J\n",
    "\n",
    "def del_J(theta, X, Y, K):\n",
    "    J = 0\n",
    "    for i in range(len(Y)):\n",
    "        for k in range(1, K):\n",
    "            if Y[i] == k:\n",
    "                multiplier = 1\n",
    "            else:\n",
    "                multiplier = 0\n",
    "            J += X[i]*(multiplier - P(theta, X))\n",
    "    return J\n",
    "\n",
    "def grad(X, Y, K, lr, epochs):\n",
    "    theta = np.random.random(size=X.shape)\n",
    "    for _ in range(epochs):\n",
    "        theta -= lr*del_J(theta, X, Y, K)\n",
    "    return theta\n",
    "\n",
    "def fit(X, Y, lr=0.01, epochs=25): \n",
    "    #Note: after 25 epochs data points get closer and closer to nan so crashes. This is fine for learning purposes\n",
    "    y, K = OneHotEncode(Y)\n",
    "    thetas = grad(X, y, K, lr, epochs)\n",
    "    return thetas\n",
    "\n",
    "def predict(X, thetas):\n",
    "    return np.round(P(thetas, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "0.9736842105263158\n",
      "Confusion Matrix : \n",
      " [[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n",
      "Accuracy :  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "X, y = load_iris(return_X_y=True)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split( \n",
    "        X, y, test_size = 0.25, random_state = 0) \n",
    "clf = LogisticRegression(random_state=0).fit(xtrain, ytrain)\n",
    "y_pred = clf.predict(xtest)\n",
    "clf.predict_proba(xtrain[:2, :])\n",
    "print(clf.score(xtest, ytest))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(ytest, y_pred) \n",
    "\n",
    "print (\"Confusion Matrix : \\n\", cm) \n",
    "from sklearn.metrics import accuracy_score \n",
    "print (\"Accuracy : \", accuracy_score(ytest, y_pred)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><b>Implementing Simple Linear Regression From Scratch (with Gradient Descent)</b> </h3>\n",
    "<h5 style=\"text-align: center;\"> This notebook is adapted and adds onto this wonderful tutorial by Arseniy Tyurin found here:\n",
    "<br><a href=\"https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc/\" target=\"_blank\">https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc/</a></h5>\n",
    "\n",
    "<h5 style=\"text-align: center;\"> <a href=\"https://en.wikipedia.org/wiki/Simple_linear_regression\" target=\"_blank\">https://en.wikipedia.org/wiki/Simple_linear_regression</a></h5>\n",
    "<h5 style=\"text-align: center;\"> <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\">https://en.wikipedia.org/wiki/Gradient_descent</a></h5>\n",
    "<h5 style=\"text-align: center;\">This notebook is similiar to Notebook 1, except it uses Gradient Descent</h5>\n",
    "<h5 style=\"text-align: center;\">We still have <code>y = &alpha; x + &beta; </code></h5>\n",
    "<h5 style=\"text-align: center;\">But now we estimate the coeffiecients not by rewriting the formula, but by writing it in terms of a function f(&alpha;,&beta;) where &alpha; and &beta; are that in the formula.</h5>\n",
    "<h5 style=\"text-align: center;\">The formula we are going to make as a function is Mean Squared Error which is the square of RMSE we explored in the previous notebook</h5>\n",
    "$$ \\text{MSE = } \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$\n",
    "$$ \\text{Where } \\hat{y_i} \\text{ we know to be } \\hat{y_i} = \\alpha x + \\beta $$\n",
    "$$ \\text{This derives to } f(\\alpha, \\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\alpha x_i - \\beta)^2 $$\n",
    "$$ \\text{We want to take the min of this; so, } \\min_{\\alpha, \\beta}f(\\alpha, \\beta) $$\n",
    "$$ \\text{We now want to take the partial deriv with respect to } \\alpha \\text{ and } \\beta $$\n",
    "$$ \\frac{\\partial f}{\\partial \\alpha} = \\frac{1}{n} \\sum_{i=1}^{n} -2x_i(y_i - \\alpha x_i - \\beta) \\text{ and} $$\n",
    "<h5 style=\"text-align: center;\">In code:<code>(-2*sum(X*(Y - alpha*X - beta))) / n</code></h5>\n",
    "$$ \\frac{\\partial f}{\\partial \\beta} = \\frac{1}{n} \\sum_{i=1}^{n} -2(y_i - \\alpha x_i - \\beta) $$\n",
    "<h5 style=\"text-align: center;\">In code:<code>(-2*sum((Y - alpha*X - beta))) / n</code></h5>\n",
    "<h5 style=\"text-align: center;\">We then multiply it by some learning rate and subtract it from the previous alpha and beta</h5><h5 style=\"text-align: center;\">The reason we subtract is because we are trying to minimze the error function. If we wanted to maximize it we would add the partials to the previous alpha and beta. <a href=\"https://medium.com/@aerinykim/why-do-we-subtract-the-slope-a-in-gradient-descent-73c7368644fa\" target=\"_blank\">This post</a> by Aerin Kim goes into a little more detail.</h5>\n",
    "<h5 style=\"text-align: center;\">We do this for e number of epochs and pretty soon we converge to an optimal minima.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9966594980280533 1.0075937461661377\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_descent_for_linear_regression(X, Y, lr=0.05, epochs=1000):\n",
    "    alpha = random()\n",
    "    beta = random()\n",
    "    n = len(X)\n",
    "    mse_details = []\n",
    "    for _ in range(epochs):\n",
    "        partial_of_alpha = (-2*sum(X*(Y - alpha*X - beta))) / n\n",
    "        partial_of_beta = (-2*sum((Y - alpha*X - beta))) / n\n",
    "        alpha -= lr*partial_of_alpha\n",
    "        beta -= lr*partial_of_beta\n",
    "        mse_details.append(sum((Y - alpha*X - beta)**2)/n)\n",
    "    print(alpha, beta)\n",
    "    return alpha, beta, mse_details\n",
    "\n",
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([2, 3, 4])\n",
    "\n",
    "epochs = 200\n",
    "alpha, beta, mse = gradient_descent_for_linear_regression(X, Y, epochs=epochs)\n",
    "\n",
    "epochs_array = [x for x in range(epochs)]\n",
    "plt.plot(epochs_array, mse)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "TODO: For some reason if a lot more data is added the program overflows. I think this is because \n",
    " the way the program is mulitplying is very ineffienct so in real life these should be numpy multiplications \n",
    " which should be far more efficient. For this demo tho that is not necessary (see issue 2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionUtils:\n",
    "    def coefficients(self, X, Y, lr, epochs):\n",
    "        alpha = random()\n",
    "        beta = random()\n",
    "        n = len(X)\n",
    "        for _ in range(epochs):\n",
    "            partial_of_alpha = (-2*sum(X*(Y - alpha*X - beta))) / n\n",
    "            partial_of_beta = (-2*sum((Y - alpha*X - beta))) / n\n",
    "            alpha -= lr*partial_of_alpha\n",
    "            beta -= lr*partial_of_beta\n",
    "        return alpha, beta\n",
    "    \n",
    "class LinearRegression(LinearRegressionUtils):\n",
    "    \"\"\"\n",
    "    Demo Linear Regression class using Gradient Descent\n",
    "    \"\"\"\n",
    "    def fit(self, X, Y, lr=0.05, epochs=1000):\n",
    "        \"\"\"\n",
    "        X and Y to fit, lr = learning rate, epochs = num epochs to iterate over\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.coef_, self.intercept_ = LinearRegressionUtils.coefficients(self, X=self.X, Y=self.Y, lr=self.lr, epochs=self.epochs)\n",
    "        self.predictions = []\n",
    "        for i in range(len(self.X)):\n",
    "            self.predictions.append(self.predict(self.X[i]))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.intercept_ + self.coef_ * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.0000010414609357\n",
      "Beta: 0.9999976325144981\n",
      "Prediction: 35.00003304218632\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([2, 3, 4])\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, Y)\n",
    "print(\"Alpha:\", reg.coef_)\n",
    "print(\"Beta:\", reg.intercept_)\n",
    "print(\"Prediction:\", reg.predict(34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: [[1.]]\n",
      "Beta: [1.]\n",
      "Prediction: [[35.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array(X).reshape(-1, 1)\n",
    "y = np.array(Y).reshape(-1, 1)\n",
    "reg2 = LinearRegression().fit(X, y)\n",
    "print(\"Alpha:\", reg2.coef_)\n",
    "print(\"Beta:\", reg2.intercept_)\n",
    "print(\"Prediction:\", reg2.predict(np.array(34).reshape(1, -1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

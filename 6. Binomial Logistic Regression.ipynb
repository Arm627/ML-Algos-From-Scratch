{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><b>Implementing Binomial Logistic Regression</b></h3>\n",
    "<h5 style=\"text-align: center;\">This notebook follows this wonderful tutorial by Nikhil Kumar: <a href=\"https://www.geeksforgeeks.org/understanding-logistic-regression/\" target=\"_blank\">https://www.geeksforgeeks.org/understanding-logistic-regression/</a><br></h5>\n",
    "<h4 style=\"text-align: center;\"><b>Note: most to all the description, code and some text is copied from GeeksToGeeks explanation mostly because its explained very well.</b></h4>\n",
    "<h4 style=\"text-align: center;\"><b>**To be honest I would recomend just following the GeeksToGeeks Tutorial. This notebook doesn't add anything much to that wonderful tutorial.</b></h4>\n",
    "<h5 style=\"text-align: center;\">Logistic regression is basically a supervised classification algorithm. In a classification problem, the target variable(or output), y, can take only discrete values for given set of features(or inputs), X.<br><br>Contrary to popular belief, logistic regression IS a regression model. The model builds a regression model to predict the probability that a given data entry belongs to the category numbered as “1”. Just like Linear regression assumes that the data follows a linear function, Logistic regression models the data using the sigmoid function.</h5>\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "<h5 style=\"text-align: center;\">Logistic regression becomes a classification technique only when a decision threshold is brought into the picture. The setting of the threshold value is a very important aspect of Logistic regression and is dependent on the classification problem itself.<br>The decision for the value of the threshold value is majorly affected by the values of precision and recall. Ideally, we want both precision and recall to be 1, but this seldom is the case. In case of a Precision-Recall tradeoff we use the following arguments to decide upon the thresold:<ol><li>Low Precision/High Recall: In applications where we want to reduce the number of false negatives without necessarily reducing the number false positives, we choose a decision value which has a low value of Precision or high value of Recall. For example, in a cancer diagnosis application, we do not want any affected patient to be classified as not affected without giving much heed to if the patient is being wrongfully diagnosed with cancer. This is because, the absence of cancer can be detected by further medical diseases but the presence of the disease cannot be detected in an already rejected candidate.</li><li>High Precision/Low Recall: In applications where we want to reduce the number of false positives without necessarily reducing the number false negatives, we choose a decision value which has a high value of Precision or low value of Recall. For example, if we are classifying customers whether they will react positively or negatively to a personalised advertisement, we want to be absolutely sure that the customer will react positively to the advertisemnt because otherwise, a negative reaction can cause a loss potential sales from the customer.</li></ol></h5>\n",
    "<h5 style=\"text-align: center;\">Based on the number of categories, Logistic regression can be classified as:<ol><li>binomial: target variable can have only 2 possible types: “0” or “1” which may represent “win” vs “loss”, “pass” vs “fail”, “dead” vs “alive”, etc.</li><li>multinomial: target variable can have 3 or more possible types which are not ordered(i.e. types have no quantitative significance) like “disease A” vs “disease B” vs “disease C”.</li><li>ordinal: it deals with target variables with ordered categories. For example, a test score can be categorized as:“very poor”, “poor”, “good”, “very good”. Here, each category can be given a score like 0, 1, 2, 3.</li></ol></h5>\n",
    "<h5 style=\"text-align: center;\">Let the data be a p x n matrix, where p is the number of feature variables and n is the number of observations</h5>\n",
    "$$ X = \\begin{equation} \\begin{bmatrix} 1 & x_{1,1} & \\ldots & x_{1,p} \\\\ 1 & x_{2,1} & \\ldots & x_{2,p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n,1} & \\ldots & x_{n,p} \\end{bmatrix} \\label{eq:aeqn} \\end{equation} $$\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?x_i%20%3D%20%5Cbegin%7Bbmatrix%7D%201%5C%5C%20x_%7Bi1%7D%5C%5C%20x_%7Bi2%7D%5C%5C%20.%5C%5C%20.%5C%5C%20x_%7Bip%7D%5C%5C%20%5Cend%7Bbmatrix%7D\" alt=\"\">\n",
    "$$ \\text{Then } h(x_i) = \\beta_0 + \\beta_1x_{i,1} + \\beta_2x_{i,2} + \\ldots + \\beta_px_{i,p}$$\n",
    "$$ \\text{Or can be } h(x_i) = \\beta^Tx_i $$\n",
    "$$ \\text{The reason for taking  } x_0 = 1 \\text{is pretty clear now.\n",
    "We needed to do a matrix product, but there was no\n",
    "actual  x_0 multiplied to } \\beta_0 \\text{in original hypothesis formula. So, we defined } x_0 = 1. $$\n",
    "$$ \\text{So } \\begin{equation} h(x_i) = g(B^Tx_i) = \\frac{1}{1 + e^{-\\beta^Tx_i }} \\end{equation} $$\n",
    "<h5 style=\"text-align: center;\">By the equation we know g(z) tends towards 1 as z -> &#8734;. And g(z) tends towards 0 as z -> -&#8734;. Thus its always bounded between 0 and 1.</h5>\n",
    "$$ \\text{So for 2 labels (0 and 1) for } i^{th} $$\n",
    "$$ P(y_i = 1|x_i;\\beta) = h(x_i) $$\n",
    "$$ P(y_i = 0|x_i;\\beta) = 1 - h(x_i) $$\n",
    "$$ \\text{Or: } P(y_i|x_i;\\beta) = (h(x_i))^{y_i}(1 - h(x_i))^{1 - y_i}$$\n",
    "$$ \\text{We also need likelihood which is: nothing but the probability of data(training examples), given a model and specific parameter values(here, }\\beta \\text{ ). It measures the support provided by the data for each possible value of the } \\beta \\text{. We obtain it by multiplying all } P(y_i|x_i) \\text{ for given }\\beta $$\n",
    "$$ L(\\beta) = \\prod_{i=1}^{n} P(y_i|x_i;\\beta) \\text{ or } $$\n",
    "$$ L(\\beta) = \\prod_{i=1}^{n} (h(x_i))^{y_i}(1 - h(x_i))^{1 - y_i} $$\n",
    "$$ \\text{For easier calculations: } l(\\beta) = \\log_{10}(L(\\beta)) \\text{ or }$$\n",
    "$$ l(\\beta) = \\sum_{i=1}^{n}y_i\\log_{10}(h(x_i)) + (1 - y_i)\\log_{10}(1 - h(x_i)) $$\n",
    "$$ \\text{Cost Function: } J(\\beta) = \\sum_{i=1}^{n}-y_i\\log_{10}(h(x_i)) - (1 - y_i)\\log_{10}(1 - h(x_i)) $$\n",
    "<h5 style=\"text-align: center;\">Using Gradient Descent</h5>\n",
    "$$ \\frac{\\partial J(\\beta)}{\\partial \\beta_j} = (h(x) - y)x_j $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated regression coefficients: [[  1.70474504  15.04062212 -20.47216021]]\n",
      "No. of iterations: 2612\n",
      "Correctly predicted labels: 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "All code is from https://www.geeksforgeeks.org/understanding-logistic-regression/ by Nikhil Kumar\n",
    "\"\"\"\n",
    "\n",
    "import csv \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def loadCSV(filename): \n",
    "    ''' \n",
    "    function to load dataset \n",
    "    '''\n",
    "    with open(filename,\"r\") as csvfile: \n",
    "        lines = csv.reader(csvfile) \n",
    "        dataset = list(lines) \n",
    "        for i in range(len(dataset)): \n",
    "            dataset[i] = [float(x) for x in dataset[i]]      \n",
    "    return np.array(dataset) \n",
    "\n",
    "def normalize(X): \n",
    "    ''' \n",
    "    function to normalize feature matrix, X \n",
    "    '''\n",
    "    mins = np.min(X, axis = 0) \n",
    "    maxs = np.max(X, axis = 0) \n",
    "    rng = maxs - mins \n",
    "    norm_X = 1 - ((maxs - X)/rng) \n",
    "    return norm_X \n",
    "\n",
    "def logistic_func(beta, X):\n",
    "    ''' \n",
    "    logistic(sigmoid) function \n",
    "    '''\n",
    "    return 1.0/(1 + np.exp(-np.dot(X, beta.T))) \n",
    "\n",
    "def log_gradient(beta, X, y): \n",
    "    ''' \n",
    "    logistic gradient function \n",
    "    '''\n",
    "    return np.dot((logistic_func(beta, X) - y.reshape(X.shape[0], -1)).T, X) \n",
    "\n",
    "def cost_func(beta, X, y): \n",
    "    ''' \n",
    "    cost function, J \n",
    "    '''\n",
    "    y = np.squeeze(y)  \n",
    "    final = -(y * np.log(logistic_func(beta, X))) - ((1 - y) * np.log(1 - logistic_func(beta, X))) \n",
    "    return np.mean(final) \n",
    "\n",
    "def grad_desc(X, y, beta, lr=.01, converge_change=.001): \n",
    "    ''' \n",
    "    gradient descent function \n",
    "    '''\n",
    "    cost = cost_func(beta, X, y) \n",
    "    change_cost = 1\n",
    "    num_iter = 1\n",
    "      \n",
    "    while(change_cost > converge_change): \n",
    "        old_cost = cost \n",
    "        beta -= lr * log_gradient(beta, X, y)\n",
    "        cost = cost_func(beta, X, y) \n",
    "        change_cost = old_cost - cost \n",
    "        num_iter += 1\n",
    "      \n",
    "    return beta, num_iter\n",
    "\n",
    "def pred_values(beta, X): \n",
    "    ''' \n",
    "    function to predict labels \n",
    "    '''\n",
    "    pred_prob = logistic_func(beta, X) \n",
    "    pred_value = np.where(pred_prob >= .5, 1, 0) \n",
    "    return np.squeeze(pred_value)\n",
    "\n",
    "def plot_reg(X, y, beta): \n",
    "    ''' \n",
    "    function to plot decision boundary \n",
    "    '''\n",
    "    # labelled observations \n",
    "    x_0 = X[np.where(y == 0.0)] \n",
    "    x_1 = X[np.where(y == 1.0)] \n",
    "      \n",
    "    # plotting points with diff color for diff label \n",
    "    plt.scatter([x_0[:, 1]], [x_0[:, 2]], c='b', label='y = 0') \n",
    "    plt.scatter([x_1[:, 1]], [x_1[:, 2]], c='r', label='y = 1') \n",
    "      \n",
    "    # plotting decision boundary \n",
    "    x1 = np.arange(0, 1, 0.1) \n",
    "    x2 = -(beta[0,0] + beta[0,1]*x1)/beta[0,2] \n",
    "    plt.plot(x1, x2, c='k', label='reg line') \n",
    "  \n",
    "    plt.xlabel('x1') \n",
    "    plt.ylabel('x2') \n",
    "    plt.legend() \n",
    "    plt.show() \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    dataset = loadCSV('Data\\\\binary_data.csv') \n",
    "      \n",
    "    # normalizing feature matrix \n",
    "    X = normalize(dataset[:, :-1]) \n",
    "      \n",
    "    # stacking columns wth all ones in feature matrix \n",
    "    X = np.hstack((np.matrix(np.ones(X.shape[0])).T, X)) \n",
    "  \n",
    "    # response vector \n",
    "    y = dataset[:, -1] \n",
    "  \n",
    "    # initial beta values \n",
    "    beta = np.matrix(np.zeros(X.shape[1])) \n",
    "  \n",
    "    # beta values after running gradient descent \n",
    "    beta, num_iter = grad_desc(X, y, beta) \n",
    "  \n",
    "    # estimated beta values and number of iterations \n",
    "    print(\"Estimated regression coefficients:\", beta) \n",
    "    print(\"No. of iterations:\", num_iter) \n",
    "  \n",
    "    # predicted labels \n",
    "    y_pred = pred_values(beta, X) \n",
    "      \n",
    "    # number of correctly predicted labels \n",
    "    print(\"Correctly predicted labels:\", np.sum(y == y_pred)) \n",
    "      \n",
    "    # plotting regression line \n",
    "    plot_reg(X, y, beta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n",
      "[[0.27172645 0.72827355]\n",
      " [0.28587674 0.71412326]]\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dataset = loadCSV('Data\\\\binary_data.csv')\n",
    "X = normalize(dataset[:, :-1]) \n",
    "y = dataset[:, -1] \n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "print(clf.predict(X[:2, :]))\n",
    "print(clf.predict_proba(X[:2, :]))\n",
    "print(clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"text-align: center;\">Note: Gradient descent is one of the many way to estimate &beta; Basically, these are more advanced algorithms which can be easily run in Python once you have defined your cost function and your gradients. These algorithms are:<ul><li>BFGS(Broyden–Fletcher–Goldfarb–Shanno algorithm)</li><li>L-BFGS(Like BFGS but uses limited memory)</li><li>Conjugate Gradient<li></ul></h5>\n",
    "<h5 style=\"text-align: center;\">Advantages/disadvantages of using any one of these algorithms over Gradient descent:</h5><h5 style=\"text-align: center;\"><br>Advantages:<br><ul><li>Don’t need to pick learning rate</li><li>Often run faster (not always the case)</li><li>Can numerically approximate gradient for you (doesn’t always work out well)</li></ul><br>Disadvantages:<ul><li>More complex</li><li>More of a black box unless you learn the specifics</li></ul></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[12  0]\n",
      " [ 0 13]]\n",
      "Accuracy :  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part is from https://www.geeksforgeeks.org/ml-logistic-regression-using-python/\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split \n",
    "xtrain, xtest, ytrain, ytest = train_test_split( \n",
    "        X, y, test_size = 0.25, random_state = 0) \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc_x = StandardScaler() \n",
    "xtrain = sc_x.fit_transform(xtrain)  \n",
    "xtest = sc_x.transform(xtest) \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "classifier = LogisticRegression(random_state = 0) \n",
    "classifier.fit(xtrain, ytrain)\n",
    "y_pred = classifier.predict(xtest)\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(ytest, y_pred) \n",
    "  \n",
    "print (\"Confusion Matrix : \\n\", cm) \n",
    "from sklearn.metrics import accuracy_score \n",
    "print (\"Accuracy : \", accuracy_score(ytest, y_pred)) \n",
    "from matplotlib.colors import ListedColormap \n",
    "X_set, y_set = xtest, ytest \n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1,  \n",
    "                               stop = X_set[:, 0].max() + 1, step = 0.01), \n",
    "                     np.arange(start = X_set[:, 1].min() - 1,  \n",
    "                               stop = X_set[:, 1].max() + 1, step = 0.01)) \n",
    "  \n",
    "plt.contourf(X1, X2, classifier.predict( \n",
    "             np.array([X1.ravel(), X2.ravel()]).T).reshape( \n",
    "             X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green'))) \n",
    "  \n",
    "plt.xlim(X1.min(), X1.max()) \n",
    "plt.ylim(X2.min(), X2.max()) \n",
    "  \n",
    "for i, j in enumerate(np.unique(y_set)): \n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], \n",
    "                c = ListedColormap(('red', 'green'))(i), label = j) \n",
    "      \n",
    "plt.title('Classifier (Test set)') \n",
    "plt.legend() \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
